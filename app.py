import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

st.set_page_config(page_title="Outil de Maillage Interne - Analyse Automatique", layout="wide")
st.title("üîó Outil de Maillage Interne - Analyse Automatique")

# √âtape 1 : Import du diagramme HTML
st.header("√âtape 1 : Importer votre diagramme de clusters (HTML)")
uploaded_html = st.file_uploader("Importer le fichier HTML de votre diagramme (export√© depuis Screaming Frog)", type=["html"])

if uploaded_html:
    html_content = uploaded_html.read().decode("utf-8")
    st.components.v1.html(html_content, height=800, scrolling=True)

    with st.expander("‚ÑπÔ∏è Explication du diagramme de clusters"):
        st.markdown("""
Le diagramme de clusters de contenu est une repr√©sentation 2D des pages de votre site, regroup√©es selon leur proximit√© s√©mantique :
- **G√©n√©ration d‚Äôembeddings** : chaque URL re√ßoit un vecteur de signification.
- **√âchantillonnage** : un sous-ensemble est s√©lectionn√© pour la clart√© visuelle.
- **R√©duction de dimensions** : pour afficher les points sur 2 axes.
- **Clustering** : les groupes sont affich√©s en diff√©rentes couleurs pour identifier les silos th√©matiques.
""")

    # Extraction des URLs pr√©sentes dans le fichier HTML (par regex, plus robuste)
    urls = list(set(re.findall(r'https?://[^\s"\'<>]+', html_content)))
    st.success(f"{len(urls)} URLs extraites du diagramme.")

    if len(urls) == 0:
        st.warning("Aucune URL d√©tect√©e dans votre fichier HTML. V√©rifiez sa structure ou contactez votre √©quipe SEO.")
    else:
        # √âtape 2 : Analyse
        st.header("√âtape 2 : Analyse d'une URL ou d'un contenu texte")
        method = st.radio("Choisissez votre m√©thode :", ["URL", "Texte brut"])

        input_text = ""
        run_analysis = False

        if method == "URL":
            url_input = st.text_input("Entrez une URL")
            if url_input:
                try:
                    response = requests.get(url_input, timeout=10)
                    page_soup = BeautifulSoup(response.text, 'html.parser')
                    input_text = page_soup.get_text(separator=' ', strip=True)
                    st.success("Contenu r√©cup√©r√© avec succ√®s.")
                    run_analysis = True
                except Exception as e:
                    st.error(f"Erreur lors de la r√©cup√©ration de l'URL : {e}")
        else:
            input_text = st.text_area("Collez votre contenu ici")
            if st.button("Analyser le contenu"):
                run_analysis = True

        if run_analysis and input_text.strip():
            # T√©l√©chargement du contenu des URLs du diagramme
            diagram_texts = []
            for u in urls:
                try:
                    r = requests.get(u, timeout=5)
                    s = BeautifulSoup(r.text, 'html.parser')
                    text = s.get_text(separator=' ', strip=True)
                    diagram_texts.append(text)
                except Exception:
                    diagram_texts.append("")

            # Analyse TF-IDF
            corpus = diagram_texts + [input_text]
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(corpus)

            # Similarit√© avec le contenu fourni
            similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()
            df = pd.DataFrame({'url': urls, 'similarity': similarities})

            # Maillage sortant (de mon contenu vers les autres)
            outgoing_links = df.sort_values(by='similarity', ascending=False).head(10)
            outgoing_links['anchor'] = outgoing_links['url'].apply(
                lambda u: " ".join(set(input_text.lower().split()) & set(u.lower().split())) or "‚Äî"
            )

            st.subheader("üîó Maillage sortant sugg√©r√© (vers des pages du diagramme)")
            st.dataframe(outgoing_links[['url', 'similarity', 'anchor']])

            # Maillage entrant (pages qui pourraient faire un lien vers mon contenu)
            incoming_links = outgoing_links.copy()
            st.subheader("üîó Maillage entrant sugg√©r√© (depuis des pages du diagramme vers votre contenu)")
            st.dataframe(incoming_links[['url', 'similarity', 'anchor']])
else:
    st.info("Veuillez d'abord importer votre fichier HTML du diagramme.")
